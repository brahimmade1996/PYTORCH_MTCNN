{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 训练PNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5807, -0.8231, -0.6591, -0.2344])\n",
      "torch.Size([4, 3, 12, 12])\n",
      "b tensor([0.4173, 0.3892, 0.3747, 0.4014], grad_fn=<SqueezeBackward0>)\n",
      "a: tensor([True, True, True, True])\n",
      "c tensor([0.4173, 0.3892, 0.3747, 0.4014], grad_fn=<MaskedSelectBackward>)\n",
      "c tensor([True, True, True, True])\n",
      "Rnet out label shape: torch.Size([4, 2])\n",
      "Rnet out offset shape: torch.Size([4, 4])\n",
      "a torch.Size([4, 2])\n",
      "torch.Size([4, 3, 48, 48])\n",
      "Rnet out label shape: torch.Size([4, 2])\n",
      "Rnet out offset shape: torch.Size([4, 4])\n",
      "Rnet out offset shape: torch.Size([4, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysf46\\anaconda3\\envs\\SEED_AI\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x21abcd074c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入公共文件\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# add other package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tool.plotcm import plot_confusion_matrix\n",
    "\n",
    "import pdb\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "\n",
    "#torch.set_printoptions(linewidth=120)\n",
    "\n",
    "from mtcnn.mtcnn import PNet\n",
    "\n",
    "from mtcnn.mtcnn import RunBuilder\n",
    "\n",
    "from mtcnn.mtcnn import LossFn\n",
    "\n",
    "from tool.imagedb import ImageDB\n",
    "\n",
    "from tool.imagedb import TrainImageReader\n",
    "\n",
    "from tool import image_tools\n",
    "\n",
    "import datetime\n",
    "\n",
    "torch.set_grad_enabled(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(prob_cls, gt_cls):\n",
    "\n",
    "    prob_cls = torch.squeeze(prob_cls)\n",
    "    \n",
    "    gt_cls = torch.squeeze(gt_cls)\n",
    "\n",
    "    #we only need the detection which >= 0\n",
    "    mask = torch.ge(gt_cls,0)\n",
    "    #get valid element\n",
    "    valid_gt_cls = torch.masked_select(gt_cls,mask)\n",
    "    \n",
    "    valid_prob_cls = torch.masked_select(prob_cls,mask)\n",
    "    \n",
    "    size = min(valid_gt_cls.size()[0], valid_prob_cls.size()[0])\n",
    "    \n",
    "    prob_ones = torch.ge(valid_prob_cls,0.6).float()\n",
    "    \n",
    "    right_ones = torch.eq(prob_ones,valid_gt_cls).float()\n",
    "    \n",
    "    #cms = confusion_matrix(prob_ones,right_ones,[0,1])\n",
    "    \n",
    "    #print(cms)\n",
    "    \n",
    "    #names = ('0','1')\n",
    "    \n",
    "    #plot_confusion_matrix(cms, names)\n",
    "    \n",
    "    #print(prob_cls.shape,gt_cls.shape,valid_prob_cls.shape,right_ones.shape)\n",
    "\n",
    "    ## if size == 0 meaning that your gt_labels are all negative, landmark or part\n",
    "    return torch.div(torch.mul(torch.sum(right_ones),float(1.0)),float(size)) \n",
    "    ## divided by zero meaning that your gt_labels are all negative, landmark or part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotation_file = './image/imglist_anno_12.txt'\n",
    "annotation_file  = '../image/12/imglist_anno_12.txt' #'./image/wider_face/wider_face_train_bbx_gt.txt' #'./image/anno_train.txt'\n",
    "\n",
    "model_store_path = '../model/Pnet'\n",
    "\n",
    "params = OrderedDict(\n",
    "    lr = [.01]\n",
    "    ,batch_size = [2000]\n",
    "    #,device = [\"cuda\", \"cpu\"]\n",
    "    ,shuffle = [True]\n",
    ")\n",
    "\n",
    "end_epoch = 10\n",
    "\n",
    "frequent = 10\n",
    "\n",
    "#runs = RunBuilder.get_runs(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(imdb=None):\n",
    "    \n",
    "    if imdb == None:\n",
    "        imagedb = ImageDB(annotation_file)\n",
    "        imdb = imagedb.load_imdb()\n",
    "        #print(imdb.num_images)\n",
    "        imdb = imagedb.append_flipped_images(imdb)\n",
    "    \n",
    "    for run in RunBuilder.get_runs(params):\n",
    "        #create model path\n",
    "        if not os.path.exists(model_store_path):\n",
    "            os.makedirs(model_store_path)\n",
    "            \n",
    "        #create data_loader\n",
    "        train_data=TrainImageReader(imdb,12,batch_size=run.batch_size,shuffle=run.shuffle)\n",
    "        \n",
    "        #print(train_data.data[0].shape,len(train_data.data))\n",
    "        #Sprint(train_data.label[0][0])\n",
    "        \n",
    "        acc=0.0\n",
    "        \n",
    "        comment = f'-{run}'\n",
    "        \n",
    "        lossfn = LossFn()\n",
    "        \n",
    "        network = PNet()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=run.lr)\n",
    "        \n",
    "        for epoch in range(end_epoch):\n",
    "            train_data.reset() # shuffle\n",
    "            epoch_acc = 0.0\n",
    "            #for batch_idx,(image,(gt_label,gt_bbox,gt_landmark))in enumerate(train_dat)\n",
    "            for batch_idx,(image,(gt_label,gt_bbox,gt_landmark))in enumerate(train_data):\n",
    "                \n",
    "                im_tensor = [ image_tools.convert_image_to_tensor(image[i,:,:,:]) for i in range(image.shape[0]) ]\n",
    "                im_tensor = torch.stack(im_tensor)\n",
    "\n",
    "                im_tensor = Variable(im_tensor)\n",
    "                gt_label = Variable(torch.from_numpy(gt_label).float())\n",
    "\n",
    "                gt_bbox = Variable(torch.from_numpy(gt_bbox).float())\n",
    "                #gt_landmark = Variable(torch.from_numpy(gt_landmark).float())\n",
    "                \n",
    "                cls_pred, box_offset_pred = network(im_tensor)\n",
    "                \n",
    "                cls_loss = lossfn.cls_loss(gt_label,cls_pred)\n",
    "                \n",
    "                box_offset_loss = lossfn.box_loss(gt_label,gt_bbox,box_offset_pred)\n",
    "                \n",
    "                all_loss = cls_loss*1.0+box_offset_loss*0.5\n",
    "                \n",
    "                if batch_idx%frequent==0:\n",
    "                    accuracy=compute_accuracy(cls_pred,gt_label)\n",
    "                    accuracy=compute_accuracy(cls_pred,gt_label)\n",
    "                    show1 = accuracy.data.cpu().numpy()\n",
    "                    show2 = cls_loss.data.cpu().numpy()\n",
    "                    show3 = box_offset_loss.data.cpu().numpy()\n",
    "                    # show4 = landmark_loss.data.cpu().numpy()\n",
    "                    show5 = all_loss.data.cpu().numpy()\n",
    "                    print(\"%s : Epoch: %d, Step: %d, accuracy: %s, det loss: %s, bbox loss: %s, all_loss: %s, lr:%s \"%\n",
    "                          (datetime.datetime.now(),epoch,batch_idx, show1,show2,show3,show5,run.lr))\n",
    "                    epoch_acc = show1\n",
    "                #计算偏差矩阵\n",
    "                optimizer.zero_grad()\n",
    "                all_loss.backward()\n",
    "                optimizer.step()\n",
    "                pass\n",
    "            \n",
    "            pass \n",
    "            print('save modle acc:', epoch_acc)\n",
    "            torch.save(network.state_dict(), os.path.join(model_store_path,\"pnet_epoch_%d.pt\" % epoch))\n",
    "            torch.save(network, os.path.join(model_store_path,\"pnet_epoch_model_%d.pkl\" % epoch))\n",
    "        pass\n",
    "        \n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Pnet Process:...\n",
      "append flipped images to imdb 212442\n",
      "2020-05-21 22:53:14.287093 : Epoch: 0, Step: 0, accuracy: 0.0, det loss: -14795.682, bbox loss: 0.013873186, all_loss: -14795.675, lr:0.01 \n",
      "2020-05-21 22:53:22.599187 : Epoch: 0, Step: 10, accuracy: 0.0, det loss: -2938418.2, bbox loss: 95.30853, all_loss: -2938370.5, lr:0.01 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3633b221f54d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#gt_imdb = imagedb.load_imdb()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#gt_imdb = imagedb.append_flipped_images(gt_imdb)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finish....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-05f7a2a14b98>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(imdb)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mepoch_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m#for batch_idx,(image,(gt_label,gt_bbox,gt_landmark))in enumerate(train_dat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgt_bbox\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgt_landmark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mim_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mimage_tools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_image_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PYTORCH_MTCNN\\tool\\imagedb.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PYTORCH_MTCNN\\tool\\imagedb.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PYTORCH_MTCNN\\tool\\imagedb.py\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0mcur_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_from\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mimdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_from\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PYTORCH_MTCNN\\tool\\imagedb.py\u001b[0m in \u001b[0;36mget_minibatch\u001b[1;34m(imdb)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[1;31m#im = Image.open(imdb[i]['image'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('train Pnet Process:...')\n",
    "    #加载图片文件\n",
    "    #imagedb = ImageDB(annotation_file,'./image/train')\n",
    "    #gt_imdb = imagedb.load_imdb()\n",
    "    #gt_imdb = imagedb.append_flipped_images(gt_imdb)\n",
    "    train_net()\n",
    "    \n",
    "    print('finish....')\n",
    "    #print(gt_imdb[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
